{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional neural network\n",
    "Dataset and code by Lauren Hayward Sierens and Juan Carrasquilla\n",
    "This code will classify the phases of the classical Ising gauge theory\n",
    "using a neural network with a convolutional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data parameters: ###\n",
    "L           = 16    # lattice length L\n",
    "num_labels  = 2     # Number of labels (T=0 and T=infinity here)\n",
    "num_sublattices = 2 # Number of sublattices for the gauge theory lattice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters: ###\n",
    "frac_train     = 0.7    # Fraction of data used for training\n",
    "patch_size     = 2      # Size of the filters\n",
    "num_filters    = 64     # Number of output channels in the convolutional layer\n",
    "stride_xy      = 1      # The size of the jumps to take as one slides the filters across the image (the lattice length L should be divisible by stride_xy)\n",
    "nH2            = 64     # Number of hidden neurons in the fully-connected layer\n",
    "keep_prob      = 0.5    # Probability of keeping neurons in the dropout layer\n",
    "learning_rate  = 0.0001 # Learning rate for training algorithm\n",
    "minibatch_size = 2000   # Mini-batch size (N_train needs to be divisible by minibatch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other parameters: ###\n",
    "N_epochs = 25           # Number of times to iterate through all of the data\n",
    "\n",
    "seed=1\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting parameters: ###\n",
    "#Specify font sizes for plots:\n",
    "plt.rcParams['axes.labelsize']  = 10\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "plt.rcParams['xtick.labelsize'] = 8\n",
    "plt.rcParams['ytick.labelsize'] = 8\n",
    "\n",
    "plt.ion() # turn on interactive plotting mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# READ IN THE DATA SET #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data: ###\n",
    "x_all     = np.loadtxt( 'x_L%d.txt' %L, dtype='uint8' )\n",
    "y_all     = np.loadtxt( 'y_L%d.txt' %L, dtype='uint8' )\n",
    "N_configs = x_all.shape[0]\n",
    "N_spins   = x_all.shape[1]\n",
    "L         = int( np.sqrt(N_spins/2) ) #should be the same as the L variable above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([255,   1, 255,   1,   1, 255, 255,   1, 255,   1, 255,   1,   1,\n",
       "         1,   1,   1, 255, 255, 255, 255, 255, 255,   1, 255, 255, 255,\n",
       "         1,   1,   1, 255,   1,   1, 255,   1,   1, 255, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255,   1, 255, 255, 255, 255, 255, 255,   1,\n",
       "       255, 255,   1,   1,   1, 255, 255,   1, 255, 255,   1,   1,   1,\n",
       "         1,   1,   1, 255,   1, 255,   1, 255, 255, 255,   1,   1,   1,\n",
       "       255,   1,   1, 255,   1, 255,   1, 255, 255, 255, 255, 255,   1,\n",
       "         1,   1, 255,   1, 255,   1, 255,   1,   1, 255, 255,   1, 255,\n",
       "         1, 255, 255,   1,   1, 255,   1,   1,   1, 255,   1,   1,   1,\n",
       "         1, 255,   1,   1, 255, 255, 255,   1, 255, 255, 255, 255, 255,\n",
       "       255, 255, 255,   1,   1, 255, 255,   1,   1, 255, 255,   1, 255,\n",
       "       255, 255, 255,   1,   1,   1,   1,   1,   1,   1,   1, 255, 255,\n",
       "         1,   1, 255,   1, 255,   1,   1, 255,   1,   1, 255, 255,   1,\n",
       "       255, 255,   1,   1,   1, 255,   1,   1, 255,   1, 255,   1,   1,\n",
       "         1, 255, 255,   1,   1, 255,   1, 255,   1,   1,   1, 255, 255,\n",
       "       255, 255,   1, 255, 255, 255, 255, 255,   1,   1, 255,   1, 255,\n",
       "         1, 255, 255,   1, 255, 255, 255, 255,   1, 255,   1, 255, 255,\n",
       "         1,   1, 255,   1,   1,   1, 255,   1,   1, 255,   1,   1, 255,\n",
       "         1,   1,   1,   1,   1, 255, 255,   1,   1,   1, 255, 255, 255,\n",
       "       255,   1, 255, 255, 255,   1,   1,   1, 255, 255,   1, 255, 255,\n",
       "         1,   1,   1,   1, 255, 255,   1, 255, 255,   1, 255, 255, 255,\n",
       "       255, 255, 255, 255, 255, 255, 255,   1, 255,   1, 255, 255,   1,\n",
       "       255,   1,   1, 255,   1, 255,   1,   1, 255, 255, 255, 255, 255,\n",
       "         1,   1,   1, 255,   1, 255, 255, 255,   1, 255,   1, 255, 255,\n",
       "         1, 255, 255,   1, 255, 255, 255,   1,   1, 255, 255, 255, 255,\n",
       "       255, 255,   1,   1,   1, 255,   1,   1, 255,   1,   1,   1,   1,\n",
       "       255,   1,   1,   1, 255, 255, 255, 255,   1, 255,   1,   1,   1,\n",
       "         1,   1,   1, 255, 255,   1,   1, 255,   1,   1, 255,   1, 255,\n",
       "       255, 255,   1, 255,   1,   1, 255, 255, 255,   1, 255,   1, 255,\n",
       "       255, 255,   1,   1,   1, 255, 255, 255,   1,   1, 255,   1,   1,\n",
       "         1,   1,   1,   1,   1, 255, 255,   1, 255, 255, 255,   1,   1,\n",
       "         1, 255, 255,   1,   1,   1,   1, 255,   1, 255,   1,   1, 255,\n",
       "         1, 255, 255,   1,   1,   1,   1,   1, 255,   1, 255, 255,   1,\n",
       "       255,   1, 255, 255, 255, 255,   1,   1, 255,   1,   1,   1,   1,\n",
       "       255,   1,   1, 255, 255,   1, 255, 255, 255, 255,   1,   1,   1,\n",
       "       255,   1, 255, 255,   1,   1,   1,   1, 255,   1, 255,   1,   1,\n",
       "       255, 255,   1, 255,   1, 255,   1, 255, 255,   1,   1,   1, 255,\n",
       "         1,   1,   1, 255, 255,   1,   1, 255, 255, 255, 255, 255, 255,\n",
       "         1,   1, 255,   1, 255,   1, 255,   1,   1,   1,   1,   1, 255,\n",
       "       255, 255,   1, 255, 255], dtype=uint8)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data and then divide into training and validation sets: ###\n",
    "indices_shuffled = np.random.permutation(N_configs)\n",
    "x_all = x_all[indices_shuffled,:]\n",
    "y_all = y_all[indices_shuffled]\n",
    "\n",
    "N_train = int(frac_train*N_configs)\n",
    "x_train_orig = x_all[0:N_train,:]\n",
    "y_train      = y_all[0:N_train]\n",
    "\n",
    "x_validation_orig = x_all[N_train:,:]\n",
    "y_validation = y_all[N_train:]\n",
    "N_validation = x_validation_orig.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  1, -1,  1, -1,  1, -1, -1, -1,  1,  1, -1, -1,  1, -1,  1,  1,\n",
       "        1,  1, -1,  1,  1, -1,  1,  1,  1,  1, -1,  1, -1, -1, -1, -1, -1,\n",
       "       -1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,\n",
       "        1, -1, -1,  1,  1, -1,  1, -1,  1, -1,  1,  1,  1,  1,  1, -1,  1,\n",
       "       -1, -1,  1,  1, -1, -1,  1,  1, -1, -1, -1,  1, -1, -1, -1,  1,  1,\n",
       "       -1, -1,  1,  1,  1,  1,  1,  1,  1, -1,  1, -1, -1, -1,  1, -1, -1,\n",
       "       -1, -1,  1, -1, -1,  1,  1,  1, -1,  1,  1, -1, -1,  1, -1, -1, -1,\n",
       "       -1,  1, -1,  1,  1, -1, -1,  1,  1,  1, -1,  1,  1, -1,  1,  1,  1,\n",
       "        1, -1, -1, -1, -1,  1,  1, -1, -1,  1, -1, -1, -1, -1, -1,  1, -1,\n",
       "       -1, -1,  1,  1,  1, -1,  1, -1,  1, -1, -1,  1, -1, -1,  1,  1,  1,\n",
       "       -1,  1, -1,  1, -1,  1, -1,  1, -1, -1,  1,  1,  1,  1, -1,  1, -1,\n",
       "        1, -1, -1, -1,  1, -1, -1,  1,  1, -1, -1, -1, -1,  1, -1,  1,  1,\n",
       "       -1,  1, -1,  1, -1, -1, -1, -1,  1,  1, -1, -1, -1,  1, -1, -1,  1,\n",
       "       -1, -1,  1,  1,  1,  1, -1,  1, -1,  1, -1,  1, -1, -1, -1, -1, -1,\n",
       "        1, -1,  1,  1,  1, -1, -1,  1,  1, -1, -1,  1,  1,  1, -1, -1, -1,\n",
       "       -1, -1,  1,  1, -1, -1, -1,  1, -1,  1, -1, -1,  1, -1, -1,  1,  1,\n",
       "        1,  1, -1, -1, -1, -1, -1,  1,  1,  1, -1, -1, -1, -1,  1,  1,  1,\n",
       "       -1,  1, -1, -1,  1, -1,  1,  1, -1,  1, -1,  1, -1,  1,  1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1,  1, -1, -1,  1, -1,  1, -1, -1, -1,\n",
       "       -1, -1,  1, -1, -1, -1,  1, -1, -1,  1, -1, -1, -1, -1,  1,  1,  1,\n",
       "        1,  1,  1,  1, -1,  1,  1,  1, -1, -1, -1,  1, -1, -1, -1, -1,  1,\n",
       "        1,  1, -1, -1, -1,  1,  1, -1, -1, -1,  1,  1,  1,  1,  1, -1, -1,\n",
       "       -1, -1,  1, -1, -1,  1, -1, -1, -1,  1,  1, -1, -1, -1, -1, -1,  1,\n",
       "        1, -1, -1,  1,  1,  1, -1,  1, -1, -1, -1, -1,  1, -1, -1,  1,  1,\n",
       "       -1,  1,  1, -1, -1,  1, -1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1,\n",
       "       -1,  1, -1, -1,  1,  1, -1,  1, -1,  1, -1,  1, -1, -1,  1, -1,  1,\n",
       "       -1, -1,  1,  1, -1, -1, -1, -1, -1,  1,  1, -1, -1, -1, -1,  1, -1,\n",
       "        1,  1,  1, -1, -1, -1,  1, -1,  1, -1,  1,  1, -1, -1, -1,  1, -1,\n",
       "        1,  1, -1,  1,  1, -1, -1, -1, -1,  1, -1,  1, -1,  1, -1,  1,  1,\n",
       "        1,  1, -1, -1,  1, -1, -1,  1, -1,  1, -1, -1,  1,  1, -1,  1, -1,\n",
       "       -1,  1], dtype=int8)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_orig[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_train     = 14000\n",
      "N_validation = 6000\n",
      "L           = 16\n",
      "L_enlarged  = 17\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Enlarge the datapoints based on the patch size (because of periodic boundary conditions): ###\n",
    "L_enlarged = L+patch_size-1\n",
    "n0 = 2*(L_enlarged)**2\n",
    "def enlarge_data(N_samples,data_orig):\n",
    "    data_enlarged = np.zeros((N_samples,n0))\n",
    "\n",
    "    for iy in range(L):\n",
    "        data_enlarged[:,2*iy*L_enlarged:(2*iy*L_enlarged + 2*L)] = data_orig[:,2*iy*L:2*(iy+1)*L]\n",
    "        data_enlarged[:,(2*iy*L_enlarged + 2*L):2*(iy+1)*L_enlarged] = data_orig[:,2*iy*L:(2*iy*L+2*(patch_size-1))]\n",
    "    data_enlarged[:,2*L*L_enlarged:] = data_enlarged[:,0:2*L_enlarged*(patch_size-1)]\n",
    "    return data_enlarged\n",
    "\n",
    "x_train      = enlarge_data(N_train, x_train_orig)\n",
    "x_validation = enlarge_data(N_validation, x_validation_orig)\n",
    "\n",
    "print(\"N_train     = %d\\nN_validation = %d\\nL           = %d\\nL_enlarged  = %d\\n\" %(N_train,N_validation,L,L_enlarged))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEFINE THE NETWORK ARCHITECTURE #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, n0]) # Placeholder for the spin configurations\n",
    "x_reshaped = tf.reshape( x, [-1,L_enlarged,L_enlarged,num_sublattices] )\n",
    "y = tf.placeholder(tf.int32, shape=[None]) # Labels\n",
    "\n",
    "### Layer 1 (Convolutional layer): ###\n",
    "W1 = tf.Variable( tf.truncated_normal([patch_size, patch_size, num_sublattices, num_filters], mean=0.0, stddev=0.01, dtype=tf.float32) )\n",
    "b1 = tf.Variable( tf.constant(0.1,shape=[num_filters]) )\n",
    "\n",
    "# Apply the convolution (note that 'VALID' means no padding):\n",
    "z1 = tf.nn.conv2d(x_reshaped, W1, strides=[1, stride_xy, stride_xy, 1], padding='VALID') + b1\n",
    "a1 = tf.nn.relu( z1 )\n",
    "n_a1 = int( num_filters*(L/stride_xy)**2 ) # Number of outputs in the vector a1\n",
    "a1_flattened = tf.reshape( a1, [-1,n_a1])\n",
    "\n",
    "### Layer 2 (Fully-connected layer): ###\n",
    "W2 = tf.Variable( tf.truncated_normal([n_a1,nH2], mean=0.0, stddev=0.01, dtype=tf.float32) )\n",
    "b2 = tf.Variable( tf.constant(0.1,shape=[nH2]) )\n",
    "z2 = tf.matmul(a1_flattened, W2) + b2\n",
    "a2 = tf.nn.relu( z2 )\n",
    "\n",
    "# Dropout: To reduce overfitting, we apply dropout to the neurons a2 (before the final output layer).\n",
    "# We create a placeholder for the probability that a neuron's output is kept during dropout, which\n",
    "# allows us to turn dropout on during training, and turn it off during validation. TensorFlow's\n",
    "# tf.nn.dropout op automatically handles scaling neuron outputs in addition to masking them, so\n",
    "# dropout works without any additional scaling.\n",
    "keepProb_var = tf.placeholder(\"float\")\n",
    "a2_drop = tf.nn.dropout(a2, keepProb_var)\n",
    "\n",
    "### Layer 3 (Fully-connected layer): ###\n",
    "W3 = tf.Variable( tf.truncated_normal([nH2,num_labels], mean=0.0, stddev=0.01, dtype=tf.float32) )\n",
    "b3 = tf.Variable( tf.constant(0.1,shape=[num_labels]) )\n",
    "z3 = tf.matmul(a2_drop, W3) + b3\n",
    "a3 = tf.nn.softmax( z3 )\n",
    "\n",
    "### Network output: ###\n",
    "aL = a3\n",
    "\n",
    "### Cost function: ###\n",
    "y_onehot = tf.one_hot(y,depth=num_labels) # labels are converted to one-hot representation\n",
    "eps=0.0000000001 # to prevent the logs from diverging\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum( y_onehot * tf.log(aL+eps) +  (1.0-y_onehot)*tf.log(1.0-aL+eps) , reduction_indices=[1]))\n",
    "cost_func = cross_entropy\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(cost_func)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n",
      "  Training cost 1.382100\n",
      "  Validation accuracy 0.507500\n",
      "\n",
      "Epoch 1:\n",
      "  Training cost 1.380554\n",
      "  Validation accuracy 0.517667\n",
      "\n",
      "Epoch 2:\n",
      "  Training cost 1.373304\n",
      "  Validation accuracy 0.521667\n",
      "\n",
      "Epoch 3:\n",
      "  Training cost 1.358280\n",
      "  Validation accuracy 0.549000\n",
      "\n",
      "Epoch 4:\n",
      "  Training cost 1.335081\n",
      "  Validation accuracy 0.618833\n",
      "\n",
      "Epoch 5:\n",
      "  Training cost 1.288927\n",
      "  Validation accuracy 0.699667\n",
      "\n",
      "Epoch 6:\n",
      "  Training cost 1.220197\n",
      "  Validation accuracy 0.772667\n",
      "\n",
      "Epoch 7:\n",
      "  Training cost 1.097702\n",
      "  Validation accuracy 0.893333\n",
      "\n",
      "Epoch 8:\n",
      "  Training cost 0.935415\n",
      "  Validation accuracy 0.950333\n",
      "\n",
      "Epoch 9:\n",
      "  Training cost 0.722207\n",
      "  Validation accuracy 0.994833\n",
      "\n",
      "Epoch 10:\n",
      "  Training cost 0.503873\n",
      "  Validation accuracy 0.999000\n",
      "\n",
      "Epoch 11:\n",
      "  Training cost 0.315062\n",
      "  Validation accuracy 1.000000\n",
      "\n",
      "Epoch 12:\n",
      "  Training cost 0.188017\n",
      "  Validation accuracy 1.000000\n",
      "\n",
      "Epoch 13:\n",
      "  Training cost 0.109463\n",
      "  Validation accuracy 1.000000\n",
      "\n",
      "Epoch 14:\n",
      "  Training cost 0.064850\n",
      "  Validation accuracy 1.000000\n",
      "\n",
      "Epoch 15:\n",
      "  Training cost 0.039843\n",
      "  Validation accuracy 1.000000\n",
      "\n",
      "Epoch 16:\n",
      "  Training cost 0.026437\n",
      "  Validation accuracy 1.000000\n",
      "\n",
      "Epoch 17:\n",
      "  Training cost 0.018777\n",
      "  Validation accuracy 1.000000\n",
      "\n",
      "Epoch 18:\n",
      "  Training cost 0.013764\n",
      "  Validation accuracy 1.000000\n",
      "\n",
      "Epoch 19:\n",
      "  Training cost 0.010649\n",
      "  Validation accuracy 1.000000\n",
      "\n",
      "Epoch 20:\n",
      "  Training cost 0.008439\n",
      "  Validation accuracy 1.000000\n",
      "\n",
      "Epoch 21:\n",
      "  Training cost 0.006955\n",
      "  Validation accuracy 1.000000\n",
      "\n",
      "Epoch 22:\n",
      "  Training cost 0.005572\n",
      "  Validation accuracy 1.000000\n",
      "\n",
      "Epoch 23:\n",
      "  Training cost 0.004783\n",
      "  Validation accuracy 1.000000\n",
      "\n",
      "Epoch 24:\n",
      "  Training cost 0.003997\n",
      "  Validation accuracy 1.000000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "epoch_list     = []\n",
    "cost_training  = []\n",
    "acc_validation = []\n",
    "\n",
    "### Train using mini-batches for several epochs: ###\n",
    "permut = np.arange(N_train)\n",
    "num_iterations = 0\n",
    "for epoch in range(N_epochs):\n",
    "    np.random.shuffle(permut) # Randomly shuffle the indices\n",
    "    x_shuffled = x_train[permut,:]\n",
    "    y_shuffled = y_train[permut]\n",
    "\n",
    "    #Loop over all the mini-batches:\n",
    "    for b in range(0, N_train, minibatch_size):\n",
    "        x_batch = x_shuffled[b:b+minibatch_size,:]\n",
    "        y_batch = y_shuffled[b:b+minibatch_size]\n",
    "        sess.run(train_step, feed_dict={x: x_batch, y:y_batch, keepProb_var:keep_prob})\n",
    "        num_iterations = num_iterations + 1\n",
    "\n",
    "    #Print results every epoch of the training algorithm:\n",
    "    cost_train = sess.run(cost_func,feed_dict={x:x_train, y:y_train, keepProb_var:1.0})\n",
    "\n",
    "    validation_output = sess.run(aL,feed_dict={x:x_validation, y:y_validation, keepProb_var:1.0})\n",
    "    predicted_class = np.argmax(validation_output, axis=1)\n",
    "    accuracy_validation = np.mean(predicted_class == y_validation)\n",
    "\n",
    "    print( \"Epoch %d:\\n  Training cost %f\\n  Validation accuracy %f\\n\" % (epoch, cost_train, accuracy_validation) )\n",
    "\n",
    "    epoch_list.append(epoch)\n",
    "    cost_training.append(cost_train)\n",
    "    acc_validation.append(accuracy_validation)\n",
    "\n",
    "    #fig = plt.figure(1,figsize=(10,4))\n",
    "    #fig.subplots_adjust(hspace=.3,wspace=.3)\n",
    "    #plt.clf()\n",
    "    \n",
    "    ### Plot the cost function during training: ###\n",
    "    #plt.subplot(121)\n",
    "    #plt.plot(epoch_list,cost_training,'o-')\n",
    "    #plt.xlabel('Epoch')\n",
    "    #plt.ylabel('Training cost')\n",
    "    \n",
    "    ### Plot the validation accuracy: ###\n",
    "    #plt.subplot(122)\n",
    "    #plt.plot(epoch_list,acc_validation,'o-')\n",
    "    #plt.xlabel('Epoch')\n",
    "    #plt.ylabel('Validation accuracy')\n",
    "    #plt.pause(0.1)\n",
    "\n",
    "#plt.savefig('gaugeTheoryClassification_CNN_results.pdf') # Save the figure showing the results in the current directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
